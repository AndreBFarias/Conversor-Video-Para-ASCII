# ==============================================
# LUNA - Arquivo de Configuracao de Ambiente
# ==============================================
# Copie este arquivo para .env e configure suas chaves
#   cp .env.example .env
#   nano .env
#
# ATENCAO: Nunca commite o arquivo .env!
# ==============================================


# ==============================================
# API KEYS (OBRIGATORIAS)
# ==============================================

# Google Gemini API - Motor principal de IA
# Obtenha em: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=sua_chave_gemini_aqui

# ElevenLabs API - Text-to-Speech premium (opcional)
# Obtenha em: https://elevenlabs.io/
# Necessario apenas se TTS_ENGINE=elevenlabs
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=


# ==============================================
# TTS (TEXT-TO-SPEECH) - Sintese de Voz
# ==============================================

# Engine de TTS: coqui | chatterbox | elevenlabs
# - coqui: Local, qualidade alta, ~6GB VRAM
# - chatterbox: Local, mais leve, ~3GB VRAM (recomendado)
# - elevenlabs: Cloud, qualidade premium, requer API key
TTS_ENGINE=coqui

# Configuracoes globais do Coqui TTS
# Paths de voz sao definidos por entidade em src/assets/panteao/entities/{id}/config.json
COQUI_MODEL_NAME=tts_models/multilingual/multi-dataset/xtts_v2
COQUI_DEVICE=cuda

# Configuracoes globais do Chatterbox TTS
# Paths de voz sao definidos por entidade em src/assets/panteao/entities/{id}/config.json
CHATTERBOX_MODEL_NAME=chatterbox-tts
CHATTERBOX_DEVICE=cuda


# ==============================================
# STT (SPEECH-TO-TEXT) - Transcricao de Voz
# ==============================================

# Motor de transcricao (apenas whisper suportado)
TRANSCRIPTION_MODEL=whisper

# Modelo Whisper: tiny | base | small | medium | large
# - tiny: ~75MB, rapido, menos preciso
# - small: ~250MB, bom custo-beneficio
# - medium: ~750MB, recomendado (default)
# - large: ~3GB, mais preciso, lento
WHISPER_MODEL_SIZE=medium

# Tipo de computacao: float16 | int8 | float32
# float16: Recomendado para GPU (mais rapido)
# int8: Recomendado para CPU (menor memoria)
WHISPER_COMPUTE_TYPE=float16

# Parametros de transcricao (ajuste fino)
WHISPER_BEAM_SIZE=3
WHISPER_BEST_OF=1

# Anti-alucinacao: valores mais altos = mais restritivo
# NO_SPEECH_THRESHOLD: 0.6-0.9 (maior = rejeita mais silencio)
WHISPER_NO_SPEECH_THRESHOLD=0.85
# COMPRESSION_RATIO: 2.0-2.4 (menor = rejeita texto repetitivo)
WHISPER_COMPRESSION_RATIO=2.2
# LOG_PROB_THRESHOLD: -1.0 a 0.0 (maior = rejeita baixa confianca)
WHISPER_LOG_PROB_THRESHOLD=-0.5
# CONDITION_ON_PREVIOUS: false evita loops de alucinacao
WHISPER_CONDITION_ON_PREVIOUS=false

# VAD integrado do Whisper
WHISPER_VAD_FILTER=true
WHISPER_VAD_MIN_SILENCE_MS=300


# ==============================================
# AUDIO - Configuracao de Dispositivos
# ==============================================

# Taxa de amostragem em Hz
# Use a taxa nativa do seu microfone (geralmente 44100 ou 48000)
# O Whisper fara resampling automatico para 16000Hz
AUDIO_SAMPLE_RATE=48000

# ID do dispositivo de audio (microfone)
# Para listar dispositivos disponiveis:
#   python -c "import pyaudio; p = pyaudio.PyAudio(); [print(f'[{i}] {p.get_device_info_by_index(i)[\"name\"]}') for i in range(p.get_device_count()) if p.get_device_info_by_index(i)['maxInputChannels'] > 0]; p.terminate()"
# Prefira dispositivos hw:X,0 (hardware direto) em vez de pulse/pipewire
AUDIO_DEVICE_ID=5

# Numero de canais (1=mono, 2=stereo)
AUDIO_CHANNELS=1

# Tamanho do chunk de audio em samples
AUDIO_CHUNK_SIZE=1024

# Duracao do buffer circular em segundos
AUDIO_BUFFER_DURATION=10.0


# ==============================================
# VAD (VOICE ACTIVITY DETECTION) - Deteccao de Fala
# ==============================================

# Tempo de silencio para considerar fim da fala (segundos)
VAD_SILENCE_DURATION=2.0

# Threshold de energia RMS para detectar fala
# Valores tipicos: 1500-4000 (maior = menos sensivel a ruido)
# - Se a Luna nao detecta sua voz, diminua para 1500-2000
# - Se ruido ambiente dispara VAD, aumente para 3000-4000
# DICA: Execute teste de microfone para ver seu RMS tipico
VAD_ENERGY_THRESHOLD=2000

# Estrategia de VAD: energy | webrtc
VAD_STRATEGY=energy

# Modo WebRTC VAD (0-3): 0=menos agressivo, 3=mais agressivo
VAD_MODE=2

# Tamanho do buffer de frames para suavizacao
VAD_FRAME_BUFFER_SIZE=6

# Limite de frames silenciosos antes de parar
VAD_SILENCE_FRAME_LIMIT=12

# Minimo de chunks de fala para considerar valido
VAD_MIN_SPEECH_CHUNKS=3

# Silencio inicial maximo antes de desistir (segundos)
VAD_MAX_INITIAL_SILENCE=4.0

# Taxa de amostragem alvo para transcodificacao
VAD_TARGET_RATE=16000


# ==============================================
# LLM (GEMINI) - Modelo de Linguagem
# ==============================================

# Modelo Gemini: gemini-2.5-flash | gemini-2.5-pro | gemini-2.0-flash
# NOTA: Modelos 1.5 foram descontinuados no novo SDK google-genai
GEMINI_MODEL=gemini-2.0-flash

# Tentativas de retry em caso de erro
GEMINI_MAX_RETRIES=3

# Delay entre retries (segundos)
GEMINI_RETRY_DELAY=2

# Timeout de requisicao (segundos) - menor = resposta mais rapida ou falha
GEMINI_TIMEOUT=15

# Tamanho do cache de respostas
GEMINI_CACHE_SIZE=50


# ==============================================
# RATE LIMITING - Controle de Quota
# ==============================================

# Limite total de requests por janela
RATE_QUOTA_LIMIT=60

# Margem de seguranca antes de bloquear
RATE_SAFETY_MARGIN=5

# Janela de tempo em segundos
RATE_WINDOW_SECONDS=60

# Requisicoes maximas por minuto
RATE_MAX_RPM=5


# ==============================================
# CACHE SEMANTICO - Otimizacao de Respostas
# ==============================================

# Threshold de similaridade para cache hit (0.0-1.0)
# Maior = mais preciso, menor = mais hits
CACHE_SIMILARITY_THRESHOLD=0.85

# Maximo de itens em cache
CACHE_MAX_SIZE=100

# Tempo de vida do cache em segundos (3600 = 1 hora)
CACHE_TTL_SECONDS=3600


# ==============================================
# FILAS (QUEUES) - Buffers de Processamento
# ==============================================

QUEUE_AUDIO_INPUT=100
QUEUE_TRANSCRIPTION=50
QUEUE_PROCESSING=20
QUEUE_RESPONSE=10
QUEUE_TTS=30
QUEUE_ANIMATION=20
QUEUE_VISION=5


# ==============================================
# MONITOR - Monitoramento de Sistema
# ==============================================

# Intervalo de verificacao de saude em segundos
MONITOR_INTERVAL=30.0

# Timeout para operacoes de thread
THREAD_TIMEOUT=5.0


# ==============================================
# UI (INTERFACE) - Configuracao Visual
# ==============================================

# FPS das animacoes ASCII (padrao para todas)
LUNA_ANIM_FPS=24.0
# FPS especifico para animacao idle (observando)
LUNA_ANIM_FPS_OBSERVANDO=24.0
# FPS especifico para modo visao (olhar)
LUNA_ANIM_FPS_VER=24.0

# Familia de fonte para renderizacao
LUNA_ANIM_FONT_FAMILY=Fira Code

# Tamanho base da fonte
LUNA_ANIM_FONT_SIZE=6

# Alinhamento vertical
LUNA_ANIM_ALIGN_V=0

# FPS do visualizador de audio (default: 24)
# Valores: 15 (economico), 24 (recomendado), 30 (fluido)
# Maior = mais CPU, menor = mais economico
VIZ_FPS=24

# Intervalo de atualizacao do visualizador de audio (legado)
VIZ_UPDATE_INTERVAL=0.1


# ==============================================
# VISAO - Webcam e Reconhecimento
# ==============================================

# Indice da webcam (0 = primeira, 1 = segunda, etc)
WEBCAM_INDEX=0

# Fonte para renderizacao ASCII da webcam
WEBCAM_FONT_FAMILY=Fira Code
WEBCAM_FONT_SIZE=8
WEBCAM_ASCII_WIDTH=70


# ==============================================
# GPU E PERFORMANCE
# ==============================================

# IMPORTANTE: Por padrao, Luna usa CPU para evitar crashes de cuDNN
# Para usar GPU, defina as variaveis abaixo ou use: ./run_luna.sh --gpu

# Forcar uso de GPU (deixe vazio para CPU, ou "0" para primeira GPU)
# Se seu CUDA/cuDNN estiver com problemas, deixe vazio
LUNA_GPU_DEVICE=

# Usar GPU para Whisper/embeddings (true/false)
# Requer CUDA e cuDNN compativeis
LUNA_USE_GPU=false

# Device para Coqui TTS (cpu | cuda)
COQUI_DEVICE=cpu

# Device para Chatterbox TTS (cpu | cuda)
CHATTERBOX_DEVICE=cpu

# Indice da GPU (0 = primeira)
GPU_DEVICE=0

# Modo de latencia: low | balanced | high
LATENCY_MODE=balanced


# ==============================================
# ARMAZENAMENTO
# ==============================================

# Salvar audio capturado para debug
SAVE_AUDIO=false

# Salvar transcricoes para analise
SAVE_TRANSCRIPTIONS=false

# Caminho do banco de dados
DATABASE_PATH=./data/transcriptions.db

# Caminho para armazenamento de audio
AUDIO_STORAGE_PATH=./data/audio/

# Dias para reter arquivos de audio
AUDIO_RETENTION_DAYS=30


# ==============================================
# VISUALIZACAO
# ==============================================

ENABLE_VISUALIZATION=false
VISUALIZATION_MODE=terminal
VISUALIZATION_FPS=30


# ==============================================
# LOGGING
# ==============================================

# Nivel de log: DEBUG | INFO | WARNING | ERROR
LOG_LEVEL=INFO

# Arquivo de log
LOG_FILE=./logs/templo_de_luna.log

# Tamanho maximo do arquivo de log em MB
LOG_MAX_SIZE_MB=10

# Numero de backups de log a manter
LOG_BACKUP_COUNT=5


# ==============================================
# ADVANCED
# ==============================================

# Timeout de processamento geral
PROCESSING_TIMEOUT=30.0


# ==============================================
# DEBUG
# ==============================================

# Modo debug (imprime informações extras)
DEBUG_MODE=false


# ==============================================
# PERSONALIDADE E INTERFACE
# ==============================================

# Nomenclatura dos botões (liturgica por padrão)
LUNA_BTN_NOVA_CONVERSA=Confissão
LUNA_BTN_HISTORICO=Relicário
LUNA_BTN_EDITAR_ALMA=Custódia
LUNA_BTN_VER=Ver
LUNA_BTN_SAIR=Réquiem

# Nome da tela de histórico
LUNA_BIBLIOTECA_NOME=Biblioteca das Palavras Negadas

# Modelo de IA local (para uso futuro)
# Suportados: gemini | ollama | local
LUNA_AI_BACKEND=gemini

# Caminho do modelo local (se LUNA_AI_BACKEND=local)
LUNA_LOCAL_MODEL_PATH=

# URL do Ollama (se LUNA_AI_BACKEND=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2


# ═══════════════════════════════════════════════════════════════
# PROVIDERS - ARQUITETURA MULTI-LLM (LOCAL vs PAGO)
# ═══════════════════════════════════════════════════════════════
# Cada funcionalidade pode usar modelo local (gratis, offline)
# ou API paga (melhor qualidade, requer internet)
#
# local  = Ollama/Whisper local (gratis, offline)
# gemini = Google Gemini API (pago)
# openai = OpenAI API (pago)
# elevenlabs = ElevenLabs API (pago)
# deepseek = DeepSeek API (pago)


# ─────────────────────────────────────────────────────────────────
# VOZ (TTS) - Text to Speech
# ─────────────────────────────────────────────────────────────────
# Opcoes: local | elevenlabs
TTS_PROVIDER=local

# Se TTS_PROVIDER=local, engine a usar:
# coqui: Melhor qualidade, ~6GB VRAM
# chatterbox: Mais leve, ~3GB VRAM, boa expressividade
# piper: Ultra leve, CPU-friendly
TTS_LOCAL_ENGINE=coqui


# ─────────────────────────────────────────────────────────────────
# OUVIDO (STT) - Speech to Text
# ─────────────────────────────────────────────────────────────────
# Opcoes: local | openai
STT_PROVIDER=local

# Idioma para transcricao
STT_LANGUAGE=pt


# ─────────────────────────────────────────────────────────────────
# VISAO - Analise de Imagens
# ─────────────────────────────────────────────────────────────────
# Opcoes: local | gemini
# local = Ollama com moondream (leve, ~1.7GB VRAM, recomendado)
# gemini = Google Gemini Flash Vision (melhor qualidade)
VISION_PROVIDER=gemini

# Modelo local para visao (se VISION_PROVIDER=local)
# Recomendados para 4GB VRAM:
# moondream: 1.7GB, rapido, bom para descricoes simples (RECOMENDADO)
# llava-phi3: 2.9GB, melhor qualidade, mais lento
VISION_LOCAL_MODEL=moondream

# Modelo Gemini para visao (se VISION_PROVIDER=gemini)
VISION_GEMINI_MODEL=gemini-1.5-flash


# ─────────────────────────────────────────────────────────────────
# CONVERSA (Persona Luna) - Chat Principal
# ─────────────────────────────────────────────────────────────────
# Opcoes: local | gemini
# local = Ollama com dolphin-mistral (sem censura, ~4GB VRAM)
# gemini = Google Gemini Flash/Pro
CHAT_PROVIDER=gemini

# Modelo local para chat (se CHAT_PROVIDER=local)
# Recomendados para 4GB VRAM:
# dolphin-mistral: 4.1GB, uncensored, bom roleplay (PRINCIPAL)
# nous-hermes2:mistral: 4.1GB, bom para narrativa
# llama3.2:3b: 2.0GB, rapido, bom portugues (FALLBACK)
CHAT_LOCAL_MODEL=dolphin-mistral
CHAT_FALLBACK_MODEL=llama3.2:3b
CHAT_LOCAL_TEMPERATURE=0.85
CHAT_LOCAL_MAX_TOKENS=1024

# Tamanho do contexto em tokens (para documentos anexados)
# Modelos maiores suportam mais contexto
# dolphin-mistral: 8192, llama3: 8192, mistral: 32768
CHAT_LOCAL_CONTEXT=8192

# Temperatura do Gemini (se CHAT_PROVIDER=gemini)
CHAT_GEMINI_TEMPERATURE=0.9


# ─────────────────────────────────────────────────────────────────
# CODIGO - Geracao e Analise de Codigo
# ─────────────────────────────────────────────────────────────────
# Opcoes: local | deepseek
# local = Ollama com qwen2.5-coder (excelente para Python/SQL)
# deepseek = DeepSeek Coder API
CODE_PROVIDER=local

# Modelo local para codigo (se CODE_PROVIDER=local)
# Recomendados: qwen2.5-coder:7b, deepseek-coder:6.7b, codellama:7b
CODE_LOCAL_MODEL=qwen2.5-coder:7b
CODE_LOCAL_TEMPERATURE=0.3
CODE_LOCAL_MAX_TOKENS=4096

# API para codigo (se CODE_PROVIDER=deepseek)
CODE_API_PROVIDER=deepseek
CODE_API_MODEL=deepseek-coder
DEEPSEEK_API_KEY=


# ─────────────────────────────────────────────────────────────────
# OLLAMA - Configuracao Geral
# ─────────────────────────────────────────────────────────────────
# Timeout para requisicoes ao Ollama (segundos)
# Aumente para 180-300 se usar documentos anexados grandes
OLLAMA_TIMEOUT=180

# Context window (tokens) - maior = mais memoria de conversa
# 8192 recomendado, 4096 para GPU com pouca VRAM
OLLAMA_NUM_CTX=8192

# Camadas na GPU (-1 = todas, 0 = CPU, N = primeiras N camadas)
# IMPORTANTE: Se qualquer camada cair para CPU, performance cai 50x
OLLAMA_NUM_GPU=-1

# Tempo para manter modelo na VRAM (30m = 30 minutos)
# Maior = menos recarregamento, mais VRAM ocupada
OLLAMA_KEEP_ALIVE=30m


# ─────────────────────────────────────────────────────────────────
# API KEYS OPCIONAIS (para providers pagos)
# ─────────────────────────────────────────────────────────────────
# OpenAI (para STT via Whisper API)
OPENAI_API_KEY=

# API LLM alternativa (opcional)
ANTHROPIC_API_KEY=
